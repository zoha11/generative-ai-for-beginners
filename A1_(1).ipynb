{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoha11/generative-ai-for-beginners/blob/main/A1_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g8jZY5WUEZF"
      },
      "source": [
        "# Assignment 1: Introduction to the Fully Recurrent Network\n",
        "\n",
        "*Author:* Thomas Adler\n",
        "\n",
        "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
        "\n",
        "\n",
        "## Exercise 1: Numerical stability of the binary cross-entropy loss function\n",
        "\n",
        "We will use the binary cross-entropy loss function to train our RNN, which is defined as\n",
        "$$\n",
        "L_{\\text{BCE}}(\\hat y, y) = -y \\log \\hat y - (1-y) \\log (1-\\hat y),\n",
        "$$\n",
        "where $y$ is the label and $\\hat y$ is a prediction, which comes from a model (e.g. an RNN) and is usually sigmoid-activated, i.e., we have\n",
        "$$\n",
        "\\hat y = \\sigma(z) = \\frac{1}{1+e^{-z}}.\n",
        "$$\n",
        "The argument $z$ is called *logit*. For reasons of numerical stability it is better to let the model emit the logit $z$ (instead of the prediction $\\hat y$) and incorporate the sigmoid activation into the loss function. Explain why this is the case and how we can gain numerical stability by combining the two functions $L_{\\text{BCE}}(\\hat y, y)$ and $\\sigma(z)$ into one function $L(z, y) = L_{\\text{BCE}}(\\sigma(z), y)$.\n",
        "\n",
        "*Hint: Prove that $\\log(1+e^{z}) = \\log (1+e^{-|z|}) + \\max(0, z)$ and argue why the right-hand side is numerically more stable. Finally, express $L(z,y)$ in terms of that form.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkYl9v5KUEZI"
      },
      "source": [
        "########## YOUR SOLUTION HERE ##########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ6SqyfuUEZI"
      },
      "source": [
        "## Exercise 2: Derivative of the loss\n",
        "\n",
        "Calculate the derivative of the binary cross-entropy loss function $L(z, y)$ with respect to the logit $z$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1cuzQpTUEZJ"
      },
      "source": [
        "########## YOUR SOLUTION HERE ##########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCklg0GjUEZJ"
      },
      "source": [
        "## Exercise 3: Initializing the network\n",
        "Consider the fully recurrent network\n",
        "$$\n",
        "s(t) = W x(t) + R a(t-1) \\\\\n",
        "a(t) = \\tanh(s(t)) \\\\\n",
        "z(t) = V a(t) \\\\\n",
        "\\hat y(t) = \\sigma(z(t))\n",
        "$$\n",
        "for $t \\in \\mathbb{N}, x(t) \\in \\mathbb{R}^{D}, s(t) \\in \\mathbb{R}^{I}, a(t) \\in \\mathbb{R}^{I}, z(t) \\in \\mathbb{R}^K, \\hat y(t) \\in \\mathbb{R}^K$ and $W, R, V$ are real matrices of appropriate sizes and $\\hat a(0) = 0$.\n",
        "\n",
        "*Compared to the lecture notes we choose $f(x) = \\tanh(x) = (e^x - e^{-x})(e^x + e^{-x})^{-1}$ and $\\varphi(x) = \\sigma(x) = (1+e^{-x})^{-1}$. Further, we introduced an auxiliary variable $z(t)$ and transposed the weight matrices.*\n",
        "\n",
        "Write a function `init` that takes a `model` and integers $D, I, K$ as arguments and stores the matrices $W, R, V$ as members `model.W`, `model.R`, `model.V`, respectively. The matrices should be `numpy` arrays of appropriate sizes and filled with random values that are uniformly distributed between -0.01 and 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pW8pwtLUEZK"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "class Obj(object):\n",
        "    pass\n",
        "\n",
        "model = Obj()\n",
        "T, D, I, K = 10, 3, 5, 1\n",
        "\n",
        "def init(model, D, I, K):\n",
        "    ########## YOUR SOLUTION HERE ##########\n",
        "\n",
        "Obj.init = init\n",
        "model.init(D, I, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaXN4GdwUEZL"
      },
      "source": [
        "## Exercise 4: The forward pass\n",
        "Implement the forward pass for the fully recurrent network for sequence classification (many-to-one mapping). To this end, write a function `forward` that takes a `model`, a sequence of input vectors `x`, and a label `y` as arguments. The inputs will be represented as a `numpy` array of shape `(T, D)`. It should execute the behavior of the fully recurrent network and evaluate the (numerically stabilized) binary cross-entropy loss at the end of the sequence and return the resulting loss value. Store the sequence of hidden activations $(a(t))_{t=1}^T$ and the logit $z(T)$ into `model.a` and `model.z`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2azwmr3YUEZL"
      },
      "outputs": [],
      "source": [
        "def forward(model, x, y):\n",
        "    ########## YOUR SOLUTION HERE ##########\n",
        "\n",
        "Obj.forward = forward\n",
        "model.forward(np.random.uniform(-1, 1, (T, D)), 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpaSBwg2UEZL"
      },
      "source": [
        "## Exercise 5: The computational graph\n",
        "\n",
        "Visualize the computational graph of the fully recurrent network unfolded in time. The graph should show the functional dependencies of the nodes $x(t), a(t), z(t), L(z(t), y(t))$ for $t \\in \\{1, 2, 3\\}$. Use the package `networkx` in combination with `matplotlib` to draw a directed graph with labelled nodes and edges. If you need help take a look at [this guide](https://networkx.org/documentation/stable/tutorial.html). Make sure to arrange the nodes in a meaningful way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe6OwkGnUEZM"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "########## YOUR SOLUTION HERE ##########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNCWtryNUEZM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}